\chapter{Analiza problemu badawczego}
\label{cha:analiza_problemu_badawczego}

\section{Aspekt medyczny - analiza morfometrii krwi}
Krwinki białe, będące komórkami systemu odpornościowego, w zależności od funkcji pełnionej w organiźmie można podzielić na pięć grup, z których cztery mają znaczny udział procentowy w składzie krwi (\ref{tab:blood_cells_percentage}).
\begin{table}[h]
\centering
\caption[Short Heading]{Udział procentowy typów krwinek białych w składzie krwi \cite{Wheater1979FunctionalHA}.}
\label{tab:blood_cells_percentage}
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Nazwa}                               & Neutrofil & Eozynofil & Limfocyt & Monocyt \\ \hline
\textbf{Udział \%} & 54-62   & 1-6    & 25-33  & 2-10  \\ \hline
\textbf{Średnica  \textbf{$\mu$}m} & 10–12  & 10–12 & 7-15  & 15-30  \\ \hline
\end{tabular}
\end{table}

{\parindent0pt % disables indentation for all the text between { and }
Baza wykorzystana w pracy zawiera zdjęcia w każdej z tych kategorii. Najważniejsze cechy, po których można rozpoznać daną klasę to wielkość komórki, kształt oraz typ jądra komórkowego. Neurofile mają jądra podzielone na segmenty, eozynofile jądra dwupłatowe, limfocyty są okrągłe z kulistymi jądrami, a monocyty z elipsodalnymi \cite{lymphocytes_nucleus}. Rys. \ref{fig:subcaption_example} przedstawia przykładowe zjęcia pochodzące z bazy.
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[scale=0.30]{Eozynofil}
		\subcaption{\label{subfigure_a}}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[scale=0.30]{Neurofil}
		\subcaption{\label{subfigure_b}}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[scale=0.30]{Lymphocyte}
		\subcaption{\label{subfigure_c}}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[scale=0.30]{Monocyte}
		\subcaption{\label{subfigure_d}}
	\end{subfigure}
	
	\caption{Zdjęcia przedstawiające: \protect\subref{subfigure_a} eozynofil, \protect\subref{subfigure_b} neurofil,  \protect\subref{subfigure_c} limfocyt,  \protect\subref{subfigure_d} monocyt.}
	\label{fig:subcaption_example}
\end{figure}
}
\section{Głębokie sieci neuronowe}

Cechą charakterystyczną głębokich seci neuronowych (ang. \textit{deep learning neural networks}) jest uczenie się wysokopoziomowej reprezentacji wzorców. Struktura sieci składa się zwykle z kilku do kilkunastu warstw (ang. \textit{layers}), chociaż czasem zdarzają się implementacje bardzo głębokich sieci z więcej niż 1000 ukytych warstw \cite{He2015DeepRL}. Jedna z pierwszych sieci bazująca na reprezentacji wysokopoziomowej, maszyna Boltzmanna (ang. \textit{restricted boltzmann machine}, RBM), została opisana w 2006 roku. Miała tylko trzy gęsto połączone ukryte warstwy, gdzie warstwę ukrytą należy rozumieć jako część sieci nie będącą wejściem ani wyjściem z układu \cite{Hinton2006AFL}. W rozpoznawaniu obrazów początkowe warstwy służą identyfikacji ogólnych i generycznych wzorców, jak rozpoznawanie krawędzi. Im głębsza warstwa tym bardziej kształty przez nie zapamiętywane przypominają reprezentacje obiektów znane człowiekowi, na przykład oczy, nos w przypadku rozpoznawaniu twarzy.

\subsection{Zasada działania sieci neuronowych}
Sztuczna sieć neuronowa (ang. \textit{artificial neural network}, ANN) jest to układ przetwarzania danych, składający się z warstw sztucznych neuronów, połączonych synapsami o konkretnych wagach (Rys. \ref{fig:ann_visualisation}). Neurony wykonują pewne operacje matematyczne na wejściowych danych, a wynik przesyłany jest do kolejnego rzędu neuronów lub do wyjścia układu. Schemat stryktury sieci został przedstawiony na Rys. \ref{fig:ann_visualisation}).

\begin{figure}[h]
	\centering
	\centering
		\includegraphics[scale=0.35]{schemat_ann}	
	\caption{Wizualizacja przykładowej struktury sieci neurnowej.}
	\label{fig:ann_visualisation}
\end{figure}

{\parindent0pt % disables indentation for all the text between { and }
Typy sieci neuronowych z podziałem na kierunek przepływu danych:
\begin{itemize}
\item{Jednokierunkowa (ang. \textit{feedfoward}) - dane w sieci przepływają tylko w kierunku od wejścia do wyjścia, a uczenie sieci odbywa się dzięki zastosowaniu propagacji wstecznej. Do tego typu należą sieci konwolucyjne.}
\item{Rekurencyjna (ang. \textit{recurrent}) - przepływ danych między dwoma połączonymi neuronami odbywa się w dowolnym kierunku, dopuszcza są też cykle.}
\item{Ze sprzężeniem zwrotnym (ang. \textit{regulatory feedback}) - dane przepływają od wejścia do wyjścia, a w celu poprawy wag sieci stosowane jest ujemne sprzężenie zwrotne.}
\item{Samoorganizujące się mapy (ang. \textit{self organizing maps}) - dane nie przepływają przez sieć. Sieć dopasowuje się do struktury zbiorów danych, na których jest uczona.}
\end{itemize}

Funkcję, realizowaną przez całą sieć można zapisać wzorem (\ref{equ:ann_formula}) \cite{tadeusiewicz_sn}:

\begin{equation}
Y = W_k X
\label{equ:ann_formula}
\end{equation}
gdzie,
\begin{eqwhere}[2cm]
	\item[$W_k$] macierz współczynników wagowych połączeń między neuronami. Ma wymiar \textit{[k x n]}, gdzie \textit{k} - liczba warstw, \textit{n} - liczba neuronów w jednej warstwie,
	\item[$X$] wektor danych wejściowych,
	\item[$Y$] wektor sygnałów wyjściowych.
\end{eqwhere}

Celem trenowania sieci neuronowej jest dobranie wartości w macierzy \textit{$W_k$} tak, aby odwzorowała wektor X w wektor Y. 
}
\subsection{Początki sieci konwolucyjnych}
%wziąć trochę info stąd:
%https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47

Konwolucyjne sieci neuronowe (ang. \textit{convolutional neural networks}, CNN) należą do najczęściej używanych głębokich sieci neuronowych w wizji komputerowej. Zbudowane na bazie perceptonu wielowarstwowego (ang. \textit{multilayer perceton}, MLP) będącego  najpopularniejszym typem ANN w latach 80' \cite{deep_learning, Wasserman1988NeuralNI}. MLP są używane w większości modeli na końcu konwolucyjnej sieci neuronowej i może zawierać kilka tego typu warstw. Pełni rolę przekodowania wartości cech zwracanych przez warstwy konwolucyjne na klasy, do których przynależy obiekt \cite{Krizhevsky2012ImageNetCW}. Podczas gdy MLP charakteryzują się tym, że są w pełni połączone, co oznacza że każdy neuron z warstwy jest powiązany z każdym neuronem z kolejnej warstwy, CNN nie są już połączone tak gęsto. Pozwala to między innymi na ograniczenie w pewnym stopniu podatności na zjawisko nadmiernego dopasowania (ang. {overfitting}). 

{\parindent0pt % disables indentation for all the text between { and }
Powstanie sieci tego typu zostało zainspirowane budową części mózgu odpowiedzialnej za odbiór wrażeń wizyjnych - kory wzrokowej \cite{Matsugu2003SubjectIF}. Narząd ten zawiera liczne drobne i gęsto ułożone komórki nerwowe. Zajmuje trzy pola Brodmana) - obszary, na który została podzielona struktura mózgu (Rys. \ref{fig:brodmann_areas}) \cite{Brodmann1985VergleichendeLD}. 

\begin{figure}[h]
	\centering
	\centering
		\includegraphics[scale=0.6]{Brodmann_areas}	
	\caption{Struktura mózgu podzielona na pola Brodmanna z zaznaczoną korą wzrokową \cite{brodmann_areas}.}
	\label{fig:brodmann_areas}
\end{figure}

Informacja wizyjna jest przekazywana z jednego obszaru do drugiego, przy czym każdy kolejny obszar jest bardziej wyspecjalizowany niż poprzedni. Pola różnią się między sobą funkcjami, przez co neurony w danym obszarze wykonują tylko konkretne zadania. Przykładowo obszar, do którego w pierwszej kolejności trafiają informacje wizyjne, nazwany bruzdą ostrogową zachowuje lokalizację przestrzenną widzianych obiektów. Przekazuje on informację do asocjacyjnej kory wzrokowej, która z kolei jest odpowiedzialna za rozpoznawanie kształtów, rozmiarów i kolorów, a potem do innych obszarów mózgu zajmujących się kojarzeniem obiektu z jego reprezentacją w pamięci. Z kolei trzeciorzędowa kora wzrokowa (ang. \textit{middle temporal}) rozpoznaje ruch obieków, a przedczołowa (ang. \textit{dorsomedial prefrontal cortex}) ruch samego obiektu rejestrująccego obraz \cite{cnn_website}. Analogicznie działają sieci konwolucyjne, w których kolejne warstwy są odpowiedzialne za detekcję różnych typów wzorców, a dane są przekazywane między warstwami i analizowane na różnych poziomach abstrakcji.

Sieci kowolucyjne znalazły zastosowanie w rozpoznawaniu obrazów, ze względu na inwariancję względem translacji oraz zdolność uczenia się wzorców lokalnych \cite{Chollet2017DeepLW}. Dane wejściowe są w postaci tensora trójwymiarowego, a operacja konwolucji (oznaczona gwiazdką), która zachodzi w warstwach sieci może być opisana równaniem (\ref{equ:convolution_formula}), \cite{deep_learning}:

\begin{equation}
S(i,j) = (I * K)(i,j) = \sum_{m}^{}\sum_{n}^{} I(m,n)K(i-m,j-n).
\label{equ:convolution_formula}
\end{equation}
gdzie,
\begin{eqwhere}[2cm]
	\item[$I$] dane wejściowe,
	\item[$K$] jądro (ang. \textit{kernel}),
	\item[$S$] wyjście, mapa cech (ang. \textit{feature map}).
\end{eqwhere}
}

\subsection{Warstwy jako podstawowe bloki konstrukcyjne}
%https://medium.com/@gopalkalpande/biological-inspiration-of-convolutional-neural-network-cnn-9419668898ac

Neurony w sieci są pogrupowane w warstwy. Typowe obliczenia w warstwie CNN składają się z trzech etapów. W pierwszym przeprowadzane jest kilka równoległych konwolucji, których wyniki nazywamy liniowymi aktywacjami. W kolejnym etapie, nazywanym detekcyjnym, każda aktywacja liniowa poddawana jest działaniu nieliniowej funkcji aktywacji. Na koniec używana jest funkcja redukująca (ang. \textit{pooling function}) \cite{deep_learning}.

{\parindent0pt % disables indentation for all the text between { and }
%https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47
%wzorowane na artykule, ale mój przykład jest autorkski
Na Rys. \ref{fig:convolution_example} zaprezentowano przykładowe działanie konwolucji dla obrazu o głębokości 3 (np. RGB). Zastosowano filtr 3x3, z krokiem równym 2, co znaczy że filtr jest stosowany co 2 piksele. Zmniejsza to rozmiar ramki do 3x3. Wyjściem tej operacji jest mapa cech (ang. \textit{feature map}). Z każdym filtrem (inaczej jądrem, (ang. \textit{kernel}) powiązana jest wartość błędu (ang. \textit{bias)}. Nie zastosowano dopełnianie macierzy zerowymi wierszami i kolumnami na brzegach (ang. \textit{padding - valid}), więc z tego powodu także następuje redukcja rozmiaru - do 2x2. 

\begin{figure}[h]
	\centering
	\centering
		\includegraphics[scale=0.5]{convolution_use_example}	
	\caption{Przykład działania konwolucji}.
	\label{fig:convolution_example}
\end{figure}

%https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0
Po przejściu przez konwolucję macierz poddawana jest funkcji aktywacyjnej. Jej celem jest obliczenie ważonej sumy macierzy, dodanie do niej wartości błędu i zdecydowanie czy dana wartość pownna być uznana za aktywną czyli braną pod uwagę w dalszym działaniu.

Funkcja redukcyjna zastępuje wartość wyjściową w danym węźle pewną warością obliczoną na podstawie wyjść sąsiednich neuronów. W ten sposób zmiejszana jest ilość próbek, a także parametrów sieci, co zmniejsza nakład obliczeniowy i redukuje overfitting. Przykład zasosowania funkcji redukcyjnej typu max pooling znajduje się na Rys. \ref{fig:max_pooling_example}. Zastosowano podział na bloki 3x3. Dzięki tej operacji uzyskuje się niezmienność wyjścia względem małych translacji wejścia.

\begin{figure}[h]
	\centering
	\centering
		\includegraphics[scale=0.15]{max_pooling_example}	
	\caption{Przykład działania redukcji max pooling}.
	\label{fig:max_pooling_example}
\end{figure}


%https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/
Na warstwy konwolucyjne nakładane są warstwy gęsto połączone (ang. \textit{dense layers}), służące do klasyfikacji. Na  ich wejściu wymagane są dane jednowymiarowe, a wyjściem konwolucji są dane trójwymiarowe. Z tego powodu łączy się je warstwą spłaszczającą (ang. \textit{flatten layer}), która transformuje macierze cech w wektor cech.

Ostatnia warstwa w pełni połączona powinna mieć wymiar równy liczbie klas, do których jest klasyfikowany zbiór danych oraz odpowiednią funkcję aktywacyjną. Dla klasyfikacji binarnej używana jest S-funkcja, a do niebinarnej funkcja softmax \cite{Chollet2017DeepLW}.

W celu zapobiegnięcia przetrenowaniu sieci używa się warstw typu dropout, które usuwają pewne połączenia między neuronami. Dzięki temu sieć uczy się cech bardziej ogólnych oraz będzie mniej podatna na osiąganie wysokiej skuteczności na zbiorze walidacyjnym, ale niskich na zbiorze testowym \cite{Srivastava2014DropoutAS}.

Temu samemu służą warstwy normalizacji wsadowej (ang. \textit{batch normalisation}), które standaryzują dane wyjściowe z poprzedniej warstwy przez nadanie im postaci rozkładu normalnego N(0,1). W przeciwieństwie do dropoutu nie powoduje utraty niektórych informacji przez usunięcie połączeń, lecz dodaje szum do każdej funkcji aktywacyjnej. Skutkiem jej stosowania jest zwiększenie niezależności warstw od siebie, generalizacja działania sieci oraz zwiększenie współczynnika uczenia. Najlepszym podejściem jest używanie obu typów wastw zmniejszających ryzyko przetrenowania \cite{Ioffe2015BatchNA}.
}

\subsection{Analiza parametrów sieci konwolucyjnej}

Sieć konwolucyjna ma wiele parametrów, które można regulować w celu dobrania jak najlepszego sposobu działania. Parametry te mają wpływ na skuteczność i mimo, że w przypadku doboru funkcji aktywacji czy redukcji da się powiedzieć które formuły mają większe prawdopodobieństwo powodzenia, to jednak nie ma ścisłych reguł ich doboru.

{\parindent0pt % disables indentation for all the text between { and }
Parametry warstwy konwolucyjnej to ilość filtrów, ich rozmiar, krok filtracji, dopełnianie zerami bądź jego brak i dobór funkji aktywacyjnej. W przypadku warstw redukcyjnych, na przykład używanej w niniejszej pracy redukcji maksymalizującej, dobiera się wielkość bloków, krok i obecność dopełnienia zerami. Warstwa gęsto połączona ma regulowaną ilość segmentów i funkcję aktywacyjną, zaś w dropout ustala się współczynnik przepuszczalności danych.

Podczas propagacji wstecznej w fazie uczenia używany jest algorytm do znajdowania minimum funkcji błędu. Minimum to można znaleźć na wiele sposobów, jednak najskuteczniejszym sposobem do szybkiej zbieżności jest użycie ADAM lub innej techniki adaptacyjnej \cite{gradient_decent}. Poniżej przedstawiono przykładowe funkcje optymalizacyjne.

\begin{itemize}
\item Naszybszy spadek (\ref{equ:gradient_decent}),
\begin{equation}
\theta_{i+1} = \theta_i - \alpha \nabla F(\theta_i)
\label{equ:gradient_decent}
\end{equation}
gdzie,
\begin{eqwhere}[2cm]
	\item[$\theta$] argument minimalizowanej funkjcji w i-tym kroku algorytmu,
	\item[$\alpha$] szybkość uczenia (ang. \textit{learining rate}),
	\item[$F$] funkcja błędu.
\end{eqwhere}
\item najszybszy spadek z regulacją bezwładności uczenia (\ref{equ:gradient_decent_momentum}),
\begin{equation}
\theta_{i+1} = \theta_i - V_i
V_{i+1} = \gamma V_i + \alpha \nabla F(\theta_i)
\label{equ:gradient_decent_momentum}
\end{equation}
gdzie,
\begin{eqwhere}[2cm]
	\item[$\gamma$] bezwładność uczenia (ang. \textit{momentum})
\end{eqwhere}
\item adapcyjne oszacowanie momentu (ang. \textit{adaptive moment estimation}, ADAM) (\ref{equ:adam}), \cite{Kingma2014AdamAM}.
\begin{equation}
\theta_{i+1} = \theta_i - \frac{\alpha}{\sqrt{v_i} + \epsilon} m_i
\label{equ:adam}
\end{equation}
gdzie,
\begin{eqwhere}[2cm]
	\item[$m_i$] pierwszy moment (wartość oczekiwana) gradientu funkcji,
	\item[$v_i$] drugi moment (wariancja) gradientu funkcji.
\end{eqwhere}
\item apropagacja średniokwadratowa (ang. \textit{root mean square propagation}, RMSprop) (\ref{equ:rmsprop}), \cite{Kurbiel2017TrainingOD}.
\begin{equation}
\theta_{i+1} = \theta_i - \frac{\alpha}{\sqrt{v_i + \epsilon}} \nabla F(\theta_i)
\label{equ:rmsprop}
\end{equation}
\end{itemize}

Przykładowe funkcje aktywacyje:
\begin{itemize}
\item progowanie (\ref{equ:activ_thresholding}),
\begin{equation}
\begin{aligned}
Y < th, A = 0\\
Y \geq th, A = 1
\end{aligned}
\label{equ:activ_thresholding}
\end{equation}
gdzie,
\begin{eqwhere}[2cm]
	\item[$Y$] wynik sumy ważonej i błędu,
	\item[$th$] próg aktywacji,
	\item[$A$] aktywacja.
\end{eqwhere}
\item funkcja liniowa (\ref{equ:activ_linear}),
\begin{equation}
A = cY
\label{equ:activ_linear}
\end{equation}
gdzie,
\begin{eqwhere}[2cm]
	\item[$c$] stała,
\end{eqwhere}
\item S-funkcja (\ref{equ:activ_sfunction}),
\begin{equation}
A = \frac{1}{1+e^{-Y}}
\label{equ:activ_sfunction}
\end{equation}
\item ReLu (\ref{equ:activ_relu}).
\begin{equation}
A = max(0,Y)
\label{equ:activ_relu}
\end{equation}
\end{itemize}

Przykładowymi funkcjami reducyjnymi są: maksimum, minimum, średnia, norma \begin{math}L^{2}\end{math}, średnia ważona odległością od centralego piksela. Najczęściej stosowana jest jednak funkcja maksimum, gdyż daje najlepsze efekty \cite{Scherer2010EvaluationOP}.
}

\subsection{Uczenie głębokich sieci neuronowych}
%https://medium.com/free-code-camp/building-a-3-layer-neural-network-from-scratch-99239c4af5d3
Proces uczenia sieci neuronowej dzieli się na epoki. Liczba epok jest regulowalnym parametrem i od przyjętej wartości zależy jakość działania modelu. Zbyt mała liczba epok skutkuje niedotrenowaniem (model mógłby klasyfikować lepiej), a zbyt duża przetrenowaniem (model zna zbiór na którym trenował bardzo dobrze, ale słabo radzi sobie z nowymi zbiorami). Poniżej przedstawiono kolejne procesy zachodzące podczas jednej epoki.

{\parindent0pt % disables indentation for all the text between { and }
Początkowo ustalane są wagi sieci $W_k$ i błędów $b_k$ przez inicjalizację małymi liczbami losowymi. W pierwszej części treningu odbywa się propagacja w przód (ang. \textit{forward popagation)}, która polega na przejściu przez sieć w kierunku od wejścia do wyjścia i obliczeniu liniowego kroku (\ref{equ:training_linear_step}):

\begin{equation}
y_1 = X_0 W_1 + b_1
\label{equ:training_linear_step}
\end{equation}
gdzie,
\begin{eqwhere}[2cm]
	\item[$X_1$] macierz wejściowa,
	\item[$W_1$] macierz wag,
	\item[$b_1$] błąd (ang. \textit{bias}),
	\item[$y_1$] pierwszy liniowy krok.
\end{eqwhere}

Następnie zbiór liniowych kroków przechodzi przez funkcje aktywacyjne, wprowadzając do modelu cechy nieliniowe i pozwalając na reprezentację bardziej skomplikowanych odwzorowań.

Po zakończonej propagacji w przód następuje etap propagacji wstecznej (ang. \textit{backward propagation}), mający na celu poprawę wartości wag. Na podstwie funkcji błędu - różnicy między wyjściem z modelu (predykcją), a oczekiwanym wyjściem - szacuje się jakość rozwiązania. Używając pochodnej funkcji błędu względem wag minimalizuje się błąd metodą najszybszego spadku. Krok spadku jest determinowany przez parametr nazywany tempem uczenia (ang. \textit{learning rate}). 

Najczęsciej nie wszystkie dane przepływają przez sieć jednocześnie. W przypadku dużych zbiorów danych dzieli się je na mniejsze podzbiory (ang. \textit{batches}), które przepływają kolejno przez sieć. Liczebność tego typu podzbioru jest parametrem modelu i wpływa na jakość klasyfikacji. Liczba iteracji definiuje ile podzbiorów ma przejść przez sieć od wejścia do wyjścia układu i spowrotem, aby epoka została uznana za skończoną. 
}

\subsection{Używanie sieci wytrenowanych}
%https://www.analyticsvidhya.com/blog/2018/07/top-10-pretrained-models-get-started-deep-learning-part-1-computer-vision/

%oraz: https://becominghuman.ai/how-do-pretrained-models-work-11fe2f64eaa2
Trening sieci neuronowej jest procesem czasochłonnym. Co więcej, wymaga zgromadzenia odpowiedniej ilości opisanych danych, co bywa problematyczne. Z tego powodu zaczęto szukać metod, dzięki którym będzie można ten proces uprościć i stosować te same narzędzia do różnych problemów. Przenoszenie uczenia (ang. \textit{transfer learning}) jest stosowane w sieciach neronowych przez użycie pretrenowanych modeli. Tego typu model jest trenowany na dużym zbiorze danych i zawierającym nawet kilka milionów próbek i kilkadziesiąt tysięcy klas. 

{\parindent0pt % disables indentation for all the text between { and }
Korzystając z faktu, że coraz głębsze warstwy sieci uczą się i rozpoznają coraz bardziej skomplikowane i szczegółowe wzorce na obrazie można zedytować raz przetrenowany model do przeznaczenia ogólnego. Należy zamrozić początkowe warstwy - rozpoznające generyczne wzorce - aby nie nadpisać ich wag oraz na nich dołożyć kolejne warstwy mające za zadanie nauczenie się szczegółów typowych dla konkretnego zbioru zdjęć. Dzięki temu można użyć pretrenowanego modelu do rozpoznawania kształtów w bazie zdjęć niezwiązanych wcale z oryginalnym zbiorem, na którym został przetrenowany. 
}
%MobileNet:
%https://kopernio.com/viewer?doi=arXiv:1704.04861&route=6

\section{Przykłady algorytmów do klasyfikacji elementów krwi}

Podejścia do problemu klasyfikacji krwinek białych spotykane w literaturze wykazują pewne wspólne cechy charakterystyczne. W przypadku sieci trenowanych od podstaw (ang. \textit{trained from scratch}) najlepsze efekty osiągały modele bazujące na warstwach konwolucji i normalizacji wsadowej. W ten sposób osiągnięto nawet 80\% skuteczności na zbiorze testowym. Mimo, że skuteczność sieci budowanych od podstaw jest wysoka, to użycie sieci pretrenowanych daje jeszcze lepsze rezultaty, bo ponad 85\%. Poniżej przedstawiono najważniejsze informacje o wybranych, najskuteczniejszych algorytmach.

\subsection{Sieć trenowana z warstwą redukcyjną maksymalizującą}
\label{section_kaggle_1}

Przytoczony algorytm w całości bazuje na publikacji \cite{kaggle_1}. Poniżej przedstawiono najważniejsze fragmenty.
{\parindent0pt % disables indentation for all the text between { and }
%https://www.kaggle.com/drobchak1988/blood-cell-images-acc-92-val-acc-90
%75\% na testowym
%TEST: 623+620+620+624 = 2487
%TRAIN: 247+2483+2478+2499 = 9957
\begin{itemize}
\item Przygotowanie danych:

Oryginalnie baza została podzielona w stosunku 75:25 na zbiory uczący i walidacyjny. W publikcji nie ma zbioru testowego, co spowodowało brak informacji o jakości działania sieci. Z tego powodu więc eksperyment przedstawiony w artkule został przeze mnie powtórzony z dokładnością co do algorytmu, ale ze zmodyfiowanym podziałem bazy danych. Pierwotny zbiór walidacyjny o liczebności 2487 elementów podzielono w stosunku 80:20 na zbiory walidacyjny i testowy. 

\begin{figure}[h]
\begin{center}
	\begin{tikzpicture} [scale = 0.6]
		\pie [text=legend, sum = 100] 
    		{75/uczący (9957 sztuk),
     	20/walidacyjny (1990 sztuk),
     	5/testowy (497 sztuk)}
	\end{tikzpicture}
\end{center}
\caption{Podział bazy danych.}
\end{figure}


Ramki zostały przeskalowane do zakresu wartości pikseli [0:1] i rozmiaru 128x128. Następnie przetasowano każdy ze zbiorów i  podzielono je na podzbiory (ang. \textit{batches}) o liczebności 32 ramek każdy.

\item Struktura sieci:
\end{itemize}

\newsavebox\myvv
\begin{lrbox}{\myvv}
\newlength{\myminipagewidth}
\newlength{\myminipagecentering}
\setlength{\myminipagewidth}{0.9\linewidth} %change this
\setlength{\myminipagecentering}{(\linewidth-\myminipagewidth)/2}
\noindent\hspace{\myminipagecentering}\begin{minipage}{\myminipagewidth}
\begin{verbatim}
__________________________________________________________________
Layer (type)                    Output Shape         Param #                
==================================================================
input_1 (InputLayer)            (None, 128, 128, 3)  0                                        
conv2d_1 (Conv2D)               (None, 128, 128, 12) 912                    
batch_normalization_2 (BatchNor (None, 128, 128, 12) 48                      

\end{verbatim} 
\end{minipage}\end{lrbox}
\resizebox{0.75\textwidth}{!}{\usebox\myvv}

\newsavebox\myvvv
\begin{lrbox}{\myvvv}
\setlength{\myminipagewidth}{0.9\linewidth} %change this
\setlength{\myminipagecentering}{(\linewidth-\myminipagewidth)/2}
\noindent\hspace{\myminipagecentering}\begin{minipage}{\myminipagewidth}
\begin{verbatim}
__________________________________________________________________
sekwencja warstw, która powtarza się pięciokrotnie:
__________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 128, 12) 156       
batch_normalization_3 (BatchNor (None, 128, 128, 12) 48                   
conv2d_3 (Conv2D)               (None, 128, 128, 12) 156       
conv2d_4 (Conv2D)               (None, 128, 128, 12) 1308        
batch_normalization_4 (BatchNor (None, 128, 128, 12) 48                    
batch_normalization_5 (BatchNor (None, 128, 128, 12) 48                     
concatenate_1 (Concatenate)     (None, 128, 128, 24) 0       
max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 24)   0            
__________________________________________________________________
.....................

\end{verbatim} 
\end{minipage}\end{lrbox}
\resizebox{0.75\textwidth}{!}{\usebox\myvvv}

\newsavebox\myvvvv
\begin{lrbox}{\myvvvv}
\setlength{\myminipagewidth}{0.9\linewidth} %change this
\setlength{\myminipagecentering}{(\linewidth-\myminipagewidth)/2}
\noindent\hspace{\myminipagecentering}\begin{minipage}{\myminipagewidth}
\begin{verbatim}
__________________________________________________________________
conv2d_17 (Conv2D)              (None, 4, 4, 12)     444         
batch_normalization_18 (BatchNo (None, 4, 4, 12)     48             
conv2d_18 (Conv2D)              (None, 4, 4, 12)     156     
conv2d_19 (Conv2D)              (None, 4, 4, 12)     1308   
batch_normalization_19 (BatchNo (None, 4, 4, 12)     48                
batch_normalization_20 (BatchNo (None, 4, 4, 12)     48                 
concatenate_6 (Concatenate)     (None, 4, 4, 24)     0       
glob_average_pooling2d_1 (Globa (None, 24)           0             
\end{verbatim} 
\end{minipage}\end{lrbox}
\resizebox{0.75\textwidth}{!}{\usebox\myvvvv}

\newsavebox\myvvvvv
\begin{lrbox}{\myvvvvv}
\setlength{\myminipagewidth}{0.9\linewidth} %change this
\setlength{\myminipagecentering}{(\linewidth-\myminipagewidth)/2}
\noindent\hspace{\myminipagecentering}\begin{minipage}{\myminipagewidth}
\begin{verbatim}
___________________________________________________________________
dense_1 (Dense)                 (None, 4)            100    
===================================================================
Total params: 35,858
Trainable params: 35,102
Non-trainable params: 756
___________________________________________________________________
\end{verbatim} 
\end{minipage}\end{lrbox}
\resizebox{0.75\textwidth}{!}{\usebox\myvvvvv}

\begin{itemize}
\item Parametry sieci i treningu:

W warstwach konwolucyjnych użyto funkcji aktywacyjnej ReLu, z dopełnianiem ramek zerami na brzegach (ang. \textit{padding - same}) i wielkością filtra zmieniającą się od 1 do 3. Warstwy normalizacji wsadowej mają bezwładność uczenia równą 0.85. Użyte warstwy redukcyjne z funkcją maksymalizującą obiliczną na podmacierzach wielkości 2x2. Sieć zakończono typowym klasyfikatorem stosowanym w modelach do problemów niebinarnych. 

Przyjęty algorytm optymalizacji to RMSprop z szybkością uczenia \(2 \times 10^{-5}\), funkcja liczenia błędu to binarna entrpia krzyżowa (ang. \textit{binary cross entropy}). Uczenie trwało 25 epok, każda epoka trwała 248 iteracji, a walidacja 62 iteracje.
\item Wyniki eksperymentu:

Wyniki powtórzonego eksperymentu pokrywają się z wynikami w publikacji w zakresie dokładności zbioru uczącego i walidacyjnego.

\begin{table}[h]
\centering
\caption[Short Heading]{Skuteczność modelu.}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{typ zbioru}           & \textbf{treningowy} & \textbf{walidacyjny} & \textbf{testowy} \\ \hline
\textbf{skuteczność {[}\%{]}} & 92                  & 90                   & 75               \\ \hline
\end{tabular}
\end{table}

\end{itemize}
}

\subsection{Sieć trenowana z regularyzacją}
%https://www.kaggle.com/placidpanda/deep-learning-from-scratch-insights
Przytoczony algorytm w całości bazuje na publikacji \cite{kaggle_2}. Poniżej przedstawiono najważniejsze fragmenty.

{\parindent0pt % disables indentation for all the text between { and }
\begin{itemize}
\item Przygotowanie danych:

Oryginalnie baza została podzielona w stosunku 75:25 na zbiory uczący i walidacyjny. Zbiór testowy uzyskano oddzielając 10\% danych walidacyjnych.

\begin{figure}[h]
\begin{center}
	\begin{tikzpicture}[scale = 0.7]
		\pie [text=legend, sum = 100] 
    		{75/uczący (9957 sztuk),
     	22.5/walidacyjny (2239 sztuk),
     	2.5/testowy (248 sztuk)}
	\end{tikzpicture}
\end{center}
\caption{Podział bazy danych.}
\end{figure}

Ramki zostały znormalizowane do zakresu wartości pikseli [0:1] i odchylenia standardowego równego 1. Zmniejszono ich rozmiar o połowę do 160x120. Następnie podzielono je na podzbiory o liczebności 16 ramek każdy.

\item Struktura sieci:
\end{itemize}

\newsavebox\myv
\begin{lrbox}{\myv}
\setlength{\myminipagewidth}{0.9\linewidth} %change this
\setlength{\myminipagecentering}{(\linewidth-\myminipagewidth)/2}
\noindent\hspace{\myminipagecentering}\begin{minipage}{\myminipagewidth}
\begin{verbatim}
____________________________________________________________________
Layer (type)                 Output Shape              Param #   
====================================================================
input_1 (InputLayer)         (None, 120, 160, 3)       0        
\end{verbatim} 
\end{minipage}\end{lrbox}
\resizebox{0.75\textwidth}{!}{\usebox\myv}

\newsavebox\myvq
\begin{lrbox}{\myvq}
\setlength{\myminipagewidth}{0.9\linewidth} %change this
\setlength{\myminipagecentering}{(\linewidth-\myminipagewidth)/2}
\noindent\hspace{\myminipagecentering}\begin{minipage}{\myminipagewidth}
\begin{verbatim}
____________________________________________________________________
sekwencja warstw, która powtarza się czterokrotnie:
____________________________________________________________________
conv2d_1 (Conv2D)            (None, 60, 80, 16)        1216      
batch_normalization_1 (Batch (None, 60, 80, 16)        64        
dropout_1 (Dropout)          (None, 60, 80, 16)        0    
____________________________________________________________________
.....................
\end{verbatim} 
\end{minipage}\end{lrbox}
\resizebox{0.75\textwidth}{!}{\usebox\myvq}

\newsavebox\myvqq
\begin{lrbox}{\myvqq}
\setlength{\myminipagewidth}{0.9\linewidth} %change this
\setlength{\myminipagecentering}{(\linewidth-\myminipagewidth)/2}
\noindent\hspace{\myminipagecentering}\begin{minipage}{\myminipagewidth}
\begin{verbatim}
____________________________________________________________________
flatten_1 (Flatten)          (None, 320)               0         
____________________________________________________________________
sekwencja warstw, która powtarza się trzykrotnie:
____________________________________________________________________
dense_1 (Dense)              (None, 32)                10272     
dropout_5 (Dropout)          (None, 32)                0         
____________________________________________________________________
.....................
____________________________________________________________________
dense_4 (Dense)              (None, 4)                 36        
____________________________________________________________________
Total params: 16,732
Trainable params: 16,668
Non-trainable params: 64
____________________________________________________________________
\end{verbatim} 
\end{minipage}\end{lrbox}
\resizebox{0.75\textwidth}{!}{\usebox\myvqq}

\begin{itemize}
\item Parametry sieci i treningu:

Liczba filtrów wastw konwolucyjnych zmieniała się o połowę od 16 do 4 przy stałej wielkości filtra 5x5. Zastosowano dopełnianie zerami, funkcję ReLu w celu aktywacji oraz dropout na poziomie 0,2. Model był trenowany przez 200 epok.
\item Wyniki eksperymentu:

\begin{table}[h]
\centering
\caption[Short Heading]{Skuteczność modelu.}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{typ zbioru}           & \textbf{treningowy} & \textbf{walidacyjny} & \textbf{testowy} \\ \hline
\textbf{skuteczność {[}\%{]}} & 99                  & 97                   & 83               \\ \hline
\end{tabular}
\end{table}

\end{itemize}
}

\subsection{Wytrenowana architektura sieci InceptionV3}
%https://www.kaggle.com/jcruxsu/blood-cell-85-kernal-with-inception-v3
Przytoczony algorytm w całości bazuje na publikacji \cite{kaggle_3}. Poniżej przedstawiono najważniejsze fragmenty.

{\parindent0pt % disables indentation for all the text between { and }
\begin{itemize}
\item Przygotowanie danych:

Zbiór uczący i testowy z oryginalnej bazy zostały użyty w całości i bez modyfikacji. Jako zbiór testowy wykorzystano mały podzbiór dołączony do bazy, którego liczebnść stanowi niecałe 3\% zbioru walidacyjnego.

\begin{figure}[h]
\begin{center}
	\begin{tikzpicture}[scale = 0.7]
		\pie [text=legend, sum = 100] 
    		{75/uczący (9957 sztuk),
     	24.25/walidacyjny (2416 sztuk),
     	0.75/testowy (71 sztuk)}
	\end{tikzpicture}
\end{center}
\caption{Podział bazy danych.}
\end{figure}

Ramki zostały użyte w oryginalnym rozmiarze 320x240, bez normalizacji i standaryzacji wartości pikseli. Podzielono je na podzbiory o liczebności 32 ramki każdy.
\item Struktura sieci:
\end{itemize}

\newsavebox\myvfff
\begin{lrbox}{\myvfff}
\setlength{\myminipagewidth}{0.9\linewidth} %change this
\setlength{\myminipagecentering}{(\linewidth-\myminipagewidth)/2}
\noindent\hspace{\myminipagecentering}\begin{minipage}{\myminipagewidth}
\begin{verbatim}
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
_________________________________________________________________
inception_v3 (Model)         (None, 6, 8, 2048)       21802784  
global_average_pooling2d_1   (None, 2048)             0           
_________________________________________________________________
sekwencja warstw, która powtarza się trzykrotnie:
_________________________________________________________________
dense_1 (Dense)              (None, 512)               1049088     
dropout_5 (Dropout)          (None, 512)               0        
_________________________________________________________________
.....................

\end{verbatim} 
\end{minipage}\end{lrbox}
\resizebox{0.75\textwidth}{!}{\usebox\myvfff}

\newsavebox\myvff
\begin{lrbox}{\myvff}
\setlength{\myminipagewidth}{0.9\linewidth} %change this
\setlength{\myminipagecentering}{(\linewidth-\myminipagewidth)/2}
\noindent\hspace{\myminipagecentering}\begin{minipage}{\myminipagewidth}
\begin{verbatim}
_________________________________________________________________
dense_4 (Dense)              (None, 5)                10272     
_________________________________________________________________
Total params: 16,732
Trainable params: 16,668
Non-trainable params: 64
_________________________________________________________________
\end{verbatim} 
\end{minipage}\end{lrbox}
\resizebox{0.75\textwidth}{!}{\usebox\myvff}

\begin{itemize}
\item Parametry sieci i treningu:

Znaczącą różnicą jest zastosowanie klasyfikatora pięciowyjściowego, co pozwala na przypisanie do ramki klasy "żadna". W warstwach gęsto połączonych użyto aktywacji ReLu, a wartości dropoutu ustalono na od 0,7 do 0,3. Użyty optymalizator to ADAM z krokiem uczenia \(5 \times 10^{-5}\). Model był trenowany przez 30 epok.

\item Wyniki eksperymentu:

\begin{table}[h]
\centering
\caption[Short Heading]{Skuteczność modelu.}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{typ zbioru}           & \textbf{treningowy} & \textbf{walidacyjny} & \textbf{testowy} \\ \hline
\textbf{skuteczność {[}\%{]}} & 99                  & 87                   & 86               \\ \hline
\end{tabular}
\end{table}

\end{itemize}
}
%PRZY ROBIENIU WLASNEGO PRETRENOWANEGO DODAĆ BATCH NORM!
%"VGG doesn’t have a batch norm layer in it because batch normalization didn’t exist before VGG. If we train it with it from the start, the pre-trained weight will benefit from the normalization of the activations. So adding a batch norm layer actually improves ImageNet, which is cool. You can add it to dense layers, and also to convolutional layers."
%źródło:
%https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c