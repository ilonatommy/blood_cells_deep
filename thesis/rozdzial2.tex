\chapter{Analiza problemu badawczego}
\label{cha:analiza_problemu_badawczego}

\section{Aspekt medyczny - analiza elementów krwi}
Krwinki białe, będące komórkami systemu odpornościowego, w zależności od funkcji pełnionej w organiźmie można podzielić na pięć grup, z których cztery mają znaczny udział procentowy w składzie krwi.
\begin{table}[h]
\centering
\caption[Short Heading]{}
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Nazwa}                               & Neutrofil & Eozynofil & Limfocyt & Monocyt \\ \hline
\textbf{Udział \%}\cite{Wheater1979FunctionalHA} & 54-62   & 1-6    & 25-33  & 2-10  \\ \hline
\textbf{Średnica  \textbf{$\mu$}m}\cite{Wheater1979FunctionalHA} & 10–12  & 10–12 & 7-15  & 15-30  \\ \hline
\end{tabular}
\end{table}

Baza wykorzystana w pracy zawiera zdjęcia w każdej z tych kategorii. Najważniejsze cechy, po których można rozpoznać daną klasę to wielkość komórki, kształt oraz typ jądra komórkowego. Neurofile mają jądra podzielone na segmenty, eozynofile jądra dwupłatowe, limfocyty są okrągłe z kulistymi jądrami, a monocyty z elipsodalnymi \cite{lymphocytes_nucleus}. Poniżej przedstawiono przykładowe zjęcia pochodzące z bazy.

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[scale=0.30]{Eozynofil}
		\subcaption{\label{subfigure_a}}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[scale=0.30]{Neurofil}
		\subcaption{\label{subfigure_b}}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[scale=0.30]{Lymphocyte}
		\subcaption{\label{subfigure_c}}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[scale=0.30]{Monocyte}
		\subcaption{\label{subfigure_d}}
	\end{subfigure}
	
	\caption{\label{fig:subcaption_example}Zdjęcia przedstawiające \protect\subref{subfigure_a} eozynofil, \protect\subref{subfigure_b} neurofil,  \protect\subref{subfigure_c} limfocyt,  \protect\subref{subfigure_d} monocyt.}
\end{figure}

\section{Głębokie sieci neuronowe}

Sieć neuronowa jest to układ przetwarzania danych, składający się z warstw sztucznych neuronów, połączonych synapsami o konkretnych wagach. Neurony wykonują pewne operacje matematyczne na wejściowych danych, a wynik przesyłany jest do kolejnego rzędu neuronów lub do wyjścia układu. Funkcję, realizowaną przez całą sieć można zapisać wzorem \cite{tadeusiewicz_sn}:

\begin{equation}
Y = W_k X
\end{equation}
gdzie,
\begin{eqwhere}[2cm]
	\item[$W_k$] macierz współczynników wagowych połączeń między neuronami. Ma wymiar \textit{[k x n]}, gdzie \textit{k} - liczba warstw, \textit{n} - liczba neuronów w jednej warstwie,
	\item[$X$] wektor danych wejściowych,
	\item[$Y$] wektor sygnałów wyjściowych.
\end{eqwhere}

Celem trenowania sieci neuronowej jest dobranie wartości w macierzy \textit{$W_k$} tak, aby odwzorowała wektor X w wektor Y. 

\begin{figure}[h]
	\centering
	\centering
		\includegraphics[scale=0.3]{schemat_ann}	
	\caption{Wizualizacja przykładowej struktury sieci neurnowej.}
\end{figure}

Typy sieci neuronowych ze względu na kierunek przepływu danych:
\begin{itemize}
\item{Jednokierunkowa (ang. \textit{feedfoward)} - dane w sieci przepływają tylko w kierunku od wejścia do wyjścia. Do tego typu należą sieci konwolucyjne.}
\item{Rekurencyjna - przepływ danych między dwoma połączonymi neuronami odbywa się w dowolnym kierunku.}
\end{itemize}

DOKONCZYĆ to, co w komentarzu
%DOPISAĆ CZYM SĄ GŁĘBOKIE SIECI I JAKA JEST RÓŻNCA MIĘDZY NIMI A ZWYKŁYMI

\subsection{Początki sieci konwolucyjnych}
%wziąć trochę info stąd:
%https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47

Konwolucyjne sieci neuronowe (ang. \textit{convolutional neural networks}, CNN) są typem sieci neuronowej głęboko uczonej. Najczęściej używana w rozpoznawaniu obrazów, ze względu na inwariancję względem translacji oraz zdolność uczenia się wzorców lokalnych \cite{Chollet2017DeepLW}. Dane wejściowe są w postaci tensora trójwymiarowego, a operacja konwolucji (oznaczona gwiazdką), która zachodzi w sieci może być opisana równaniem \cite{deep_learning}:

\begin{equation}
S(i,j) = (I * K)(i,j) = \sum_{m}^{}\sum_{n}^{} I(m,n)K(i-m,j-n).
\end{equation}
gdzie,
\begin{eqwhere}[2cm]
	\item[$I$] dane wejściowe,
	\item[$K$] jądro (ang. \textit{kernel}),
	\item[$S$] wyjście, mapa cech (ang. \textit{feature map}).
\end{eqwhere}

DOKONCZYĆ to, co w komentarzu
%HISTORIA CNN, aby treść paragrafu pasowała do tytułu

\subsection{Warstwy sieci konwolucyjnych}
Neurony w sieci są pogrupowane w warstwy. Typowe obliczenia w warstwie CNN składają się z trzech etapów. W pierwszym przeprowadzane jest kilka równoległych konwolucji, których wyniki nazywamy liniowymi aktywacjami. W kolejnym etapie, nazywanym detekcyjnym, każda aktywacja liniowa poddawana jest działaniu nieliniowej funkcji aktywacji. Na koniec używana jest funkcja redukująca (ang. \textit{pooling function}) \cite{deep_learning}.

%https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47
%wzorowane na artykule, ale mój przykład jest autorkski
Poniżej przykładowe działanie konwolucji dla obrazu o głębokości 3 (np. RGB) z zastosowaniem filtru 3x3 z krokiem równym 2, co znaczy że filtr jest stosowany co 2 piksele. Zmniejsza to rozmiar ramki do 3x3. Wyjściem tej operacji jest mapa cech (ang. \textit{feature map}). Z każdym filtrem (inaczej jądrem, (ang. \textit{kernel}) powiązana jest wartość błędu (ang. \textit{bias)}. Nie zastosowano dopełniania macierzy zerowymi wierszami i kolumnami na brzegach (ang. \textit{padding - same}), więc z tego powodu także następuje redukcja rozmiaru - do 2x2. 

\begin{table}[h]
\centering
\caption[Short Heading]{}
\begin{tabular}{lllllllllllllllllllllll}
\multicolumn{5}{l}{wejście: (5x5x3)}                                                                                                                   &  &  &                       & \multicolumn{3}{l}{filtr: (3x3x3)}                                          &  &  &                       & \multicolumn{4}{l}{wynik: (2x2x3)}                     &                       & \multicolumn{4}{l}{wyjście: (2x2x1)}                   \\
\multicolumn{5}{l}{x{[}:,:,1{]}}                                                                                                                       &  &  &                       & \multicolumn{3}{l}{f1{[}:,:,1{]}}                                           &  &  &                       & \multicolumn{2}{l}{a1{[}:,:,1{]}}                &  &  &                       & \multicolumn{2}{l}{y{[}:,:,1{]}}                 &  &  \\ \cline{1-5} \cline{9-11} \cline{15-16} \cline{20-21}
\multicolumn{1}{|l|}{1} & \multicolumn{1}{l|}{2}          & \multicolumn{1}{l|}{3}          & \multicolumn{1}{l|}{1}          & \multicolumn{1}{l|}{1} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{1}  & \multicolumn{1}{l|}{0}  & \multicolumn{1}{l|}{-1} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{1}  & \multicolumn{1}{l|}{4} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{4} & \multicolumn{1}{l|}{12} &  &  \\ \cline{1-5} \cline{9-11} \cline{15-16} \cline{20-21}
\multicolumn{1}{|l|}{0} & \multicolumn{1}{l|}{\textbf{1}} & \multicolumn{1}{l|}{\textbf{4}} & \multicolumn{1}{l|}{\textbf{0}} & \multicolumn{1}{l|}{2} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{1}  & \multicolumn{1}{l|}{0}  & \multicolumn{1}{l|}{-1} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{4}  & \multicolumn{1}{l|}{4} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{9} & \multicolumn{1}{l|}{11} &  &  \\ \cline{1-5} \cline{9-11} \cline{15-16} \cline{20-21}
\multicolumn{1}{|l|}{1} & \multicolumn{1}{l|}{\textbf{2}} & \multicolumn{1}{l|}{\textbf{0}} & \multicolumn{1}{l|}{\textbf{2}} & \multicolumn{1}{l|}{0} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{1}  & \multicolumn{1}{l|}{0}  & \multicolumn{1}{l|}{-1} &  &  &                       &                         &                        &  &  &                       &                        &                         &  &  \\ \cline{1-5} \cline{9-11}
\multicolumn{1}{|l|}{4} & \multicolumn{1}{l|}{\textbf{0}} & \multicolumn{1}{l|}{\textbf{4}} & \multicolumn{1}{l|}{\textbf{0}} & \multicolumn{1}{l|}{0} &  &  &                       &                         & \textbf{}               & \textbf{}               &  &  &                       &                         &                        &  &  &                       &                        &                         &  &  \\ \cline{1-5}
\multicolumn{1}{|l|}{0} & \multicolumn{1}{l|}{1}          & \multicolumn{1}{l|}{1}          & \multicolumn{1}{l|}{0}          & \multicolumn{1}{l|}{1} &  &  &                       &                         &                         &                         &  &  &                       &                         &                        &  &  &                       &                        &                         &  &  \\ \cline{1-5}
\multicolumn{5}{l}{x{[}:,:,2{]}}                                                                                                                       &  &  &                       & \multicolumn{3}{l}{f1{[}:,:,2{]}}                                           &  &  &                       & \multicolumn{2}{l}{a1{[}:,:,2{]}}                &  &  &                       &                        &                         &  &  \\ \cline{1-5} \cline{9-11} \cline{15-16}
\multicolumn{1}{|l|}{0} & \multicolumn{1}{l|}{2}          & \multicolumn{1}{l|}{1}          & \multicolumn{1}{l|}{0}          & \multicolumn{1}{l|}{3} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{1}  & \multicolumn{1}{l|}{-1} & \multicolumn{1}{l|}{1}  &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{-1} & \multicolumn{1}{l|}{4} &  &  &                       &                        &                         &  &  \\ \cline{1-5} \cline{9-11} \cline{15-16}
\multicolumn{1}{|l|}{0} & \multicolumn{1}{l|}{\textbf{0}} & \multicolumn{1}{l|}{\textbf{2}} & \multicolumn{1}{l|}{\textbf{1}} & \multicolumn{1}{l|}{0} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{-1} & \multicolumn{1}{l|}{0}  & \multicolumn{1}{l|}{-1} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{1}  & \multicolumn{1}{l|}{2} &  &  &                       &                        &                         &  &  \\ \cline{1-5} \cline{9-11} \cline{15-16}
\multicolumn{1}{|l|}{1} & \multicolumn{1}{l|}{\textbf{1}} & \multicolumn{1}{l|}{\textbf{2}} & \multicolumn{1}{l|}{\textbf{0}} & \multicolumn{1}{l|}{0} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{1}  & \multicolumn{1}{l|}{-1} & \multicolumn{1}{l|}{1}  &  &  &                       &                         &                        &  &  &                       &                        &                         &  &  \\ \cline{1-5} \cline{9-11}
\multicolumn{1}{|l|}{2} & \multicolumn{1}{l|}{\textbf{0}} & \multicolumn{1}{l|}{\textbf{1}} & \multicolumn{1}{l|}{\textbf{0}} & \multicolumn{1}{l|}{0} &  &  &                       &                         &                         &                         &  &  &                       &                         &                        &  &  &                       &                        &                         &  &  \\ \cline{1-5}
\multicolumn{1}{|l|}{3} & \multicolumn{1}{l|}{1}          & \multicolumn{1}{l|}{0}          & \multicolumn{1}{l|}{0}          & \multicolumn{1}{l|}{1} &  &  &                       &                         &                         &                         &  &  &                       &                         &                        &  &  &                       &                        &                         &  &  \\ \cline{1-5}
\multicolumn{5}{l}{x{[}:,:,3{]}}                                                                                                                       &  &  &                       & \multicolumn{3}{l}{f1{[}:,:,3{]}}                                           &  &  &                       & \multicolumn{2}{l}{a1{[}:,:,3{]}}                &  &  &                       &                        &                         &  &  \\ \cline{1-5} \cline{9-11} \cline{15-16}
\multicolumn{1}{|l|}{3} & \multicolumn{1}{l|}{0}          & \multicolumn{1}{l|}{4}          & \multicolumn{1}{l|}{1}          & \multicolumn{1}{l|}{0} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{0}  & \multicolumn{1}{l|}{1}  & \multicolumn{1}{l|}{0}  &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{4}  & \multicolumn{1}{l|}{4} &  &  &                       &                        &                         &  &  \\ \cline{1-5} \cline{9-11} \cline{15-16}
\multicolumn{1}{|l|}{0} & \multicolumn{1}{l|}{\textbf{1}} & \multicolumn{1}{l|}{\textbf{2}} & \multicolumn{1}{l|}{\textbf{0}} & \multicolumn{1}{l|}{2} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{1}  & \multicolumn{1}{l|}{1}  & \multicolumn{1}{l|}{1}  &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{4}  & \multicolumn{1}{l|}{5} &  &  &                       &                        &                         &  &  \\ \cline{1-5} \cline{9-11} \cline{15-16}
\multicolumn{1}{|l|}{2} & \multicolumn{1}{l|}{\textbf{3}} & \multicolumn{1}{l|}{\textbf{1}} & \multicolumn{1}{l|}{\textbf{0}} & \multicolumn{1}{l|}{2} &  &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{-1} & \multicolumn{1}{l|}{1}  & \multicolumn{1}{l|}{0}  &  &  &                       &                         &                        &  &  &                       &                        &                         &  &  \\ \cline{1-5} \cline{9-11}
\multicolumn{1}{|l|}{0} & \multicolumn{1}{l|}{\textbf{1}} & \multicolumn{1}{l|}{\textbf{2}} & \multicolumn{1}{l|}{\textbf{4}} & \multicolumn{1}{l|}{1} &  &  &                       &                         &                         &                         &  &  &                       &                         &                        &  &  &                       &                        &                         &  &  \\ \cline{1-5}
\multicolumn{1}{|l|}{3} & \multicolumn{1}{l|}{1}          & \multicolumn{1}{l|}{3}          & \multicolumn{1}{l|}{1}          & \multicolumn{1}{l|}{0} &  &  &                       &                         &                         &                         &  &  &                       &                         &                        &  &  &                       &                        &                         &  &  \\ \cline{1-5}
\end{tabular}
\end{table}

%https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0
Po przejściu przez konwolucję macierz poddawana jest funkcji aktywacyjnej. Jej celem jest obliczenie ważonej sumy macierzy, dodanie do niej wartości błędu i zdecydowanie czy dana wartość pownna być uznana za aktywną czyli braną pod uwagę w dalszym działaniu.

Funkcja redukcyjna zastępuje wartość wyjściową w danym węźle pewną warością obliczoną na podstawie wyjść sąsiednich neuronów. W ten sposób zmiejszana jest ilość próbek, a także parametrów sieci, co zmniejsza nakład obliczeniowy i redukuje overfitting. Przykład zasosowania funkcji redkcyjnej typu max pooling:

\begin{table}[h]
\centering
\caption[Short Heading]{}
\begin{tabular}{llllllllll}
\multicolumn{6}{l}{macierz wejściowa}                                                                                                                                                                                                                                                                 &  &                       & \multicolumn{2}{l}{wynik}                                                                        \\ \cline{1-6}
\multicolumn{1}{|l|}{\cellcolor[HTML]{CBCEFB}1} & \multicolumn{1}{l|}{\cellcolor[HTML]{CBCEFB}10} & \multicolumn{1}{l|}{\cellcolor[HTML]{CBCEFB}2} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFE65}6} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFE65}6} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFE65}2} &  &                       &                                                 &                                                \\ \cline{1-6}
\multicolumn{1}{|l|}{\cellcolor[HTML]{CBCEFB}0} & \multicolumn{1}{l|}{\cellcolor[HTML]{CBCEFB}2}  & \multicolumn{1}{l|}{\cellcolor[HTML]{CBCEFB}4} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFE65}3} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFE65}5} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFE65}4} &  &                       &                                                 &                                                \\ \cline{1-6} \cline{9-10} 
\multicolumn{1}{|l|}{\cellcolor[HTML]{CBCEFB}4} & \multicolumn{1}{l|}{\cellcolor[HTML]{CBCEFB}0}  & \multicolumn{1}{l|}{\cellcolor[HTML]{CBCEFB}1} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFE65}2} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFE65}8} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFE65}1} &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{\cellcolor[HTML]{CBCEFB}10} & \multicolumn{1}{l|}{\cellcolor[HTML]{FCFF2F}8} \\ \cline{1-6} \cline{9-10} 
\multicolumn{1}{|l|}{\cellcolor[HTML]{FFCB2F}0} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFCB2F}2}  & \multicolumn{1}{l|}{\cellcolor[HTML]{FFCB2F}5} & \multicolumn{1}{l|}{\cellcolor[HTML]{32CB00}4} & \multicolumn{1}{l|}{\cellcolor[HTML]{32CB00}9} & \multicolumn{1}{l|}{\cellcolor[HTML]{32CB00}0} &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFC702}5}  & \multicolumn{1}{l|}{\cellcolor[HTML]{32CB00}9} \\ \cline{1-6} \cline{9-10} 
\multicolumn{1}{|l|}{\cellcolor[HTML]{FFCB2F}4} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFCB2F}5}  & \multicolumn{1}{l|}{\cellcolor[HTML]{FFCB2F}3} & \multicolumn{1}{l|}{\cellcolor[HTML]{32CB00}7} & \multicolumn{1}{l|}{\cellcolor[HTML]{32CB00}5} & \multicolumn{1}{l|}{\cellcolor[HTML]{32CB00}3} &  &                       &                                                 &                                                \\ \cline{1-6}
\multicolumn{1}{|l|}{\cellcolor[HTML]{FFCB2F}2} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFCB2F}1}  & \multicolumn{1}{l|}{\cellcolor[HTML]{FFCB2F}5} & \multicolumn{1}{l|}{\cellcolor[HTML]{32CB00}0} & \multicolumn{1}{l|}{\cellcolor[HTML]{32CB00}2} & \multicolumn{1}{l|}{\cellcolor[HTML]{32CB00}7} &  &                       &                                                 &                                                \\ \cline{1-6}
\end{tabular}
\end{table}

Dzięki tej operacji uzyskuje się niezmienność wyjścia względem małych translacji wejścia.

%https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/
Na warstwy konwolucyjne nakładane są warstwy gęsto połączone (ang. \textit{dense layers}), służące do klasyfikacji. Na  ich wejściu wymagane są dane jednowymiarowe, a wyjściem konwolucji są dane trójwymiarowe. Z tego powodu łączy się je warstwą spłaszczającą (ang. \textit{flatten layer}), która transformuje macierze cech w wektor cech.

Ostatnia warstwa w pełni połączona powinna mieć wymiar równy liczbie klas, do których jest klasyfikowany zbiór danych oraz odpowiednią funkcję aktywacyjną. Dla klasyfikacji binarnej używana jest S-funkcja, a do niebinarnej funkcja softmax.

\subsection{Analiza parametrów sieci konwolucyjnej}

Parametermi, które można regulować w sieci są rozmiar filtru, liczba filtrów, krok filtracji, dopełnianie zerami, dobór funkcji aktywacyjnej i redukcyjnej. Parametry te mają wpływ na skuteczność działania sieci i mimo, że w przypadku funkcji da się powiedzieć które formuły mają większe prawdopodobieństwo powodzenia, to jednak nie ma ścisłych reguł ich doboru.

Przykładowe funkcje aktywacyje:
\begin{itemize}
\item progowanie
\begin{equation*}
\begin{aligned}
Y < th, A = 0\\
Y \geq th, A = 1
\end{aligned}
\end{equation*}
gdzie,
\begin{eqwhere}[2cm]
	\item[$Y$] wynik sumy ważonej i błędu,
	\item[$th$] próg aktywacji,
	\item[$A$] aktywacja.
\end{eqwhere}
\item funkcja liniowa
\begin{equation*}
A = cY
\end{equation*}
gdzie,
\begin{eqwhere}[2cm]
	\item[$c$] stała,
\end{eqwhere}
\item S-funkcja
\begin{equation*}
A = \frac{1}{1+e^{-Y}}
\end{equation*}
\item ReLu
\begin{equation*}
A = max(0,Y)
\end{equation*}
\end{itemize}

Przykładowymi funkcjami reducyjnymi są: maksimum, minimum, średnia, norma \begin{math}L^{2}\end{math}, średnia ważona odległością od centralego piksela. Najczęściej stosowana jest jednak funkcja maksimum, gdyż daje najlepsze efekty \cite{Scherer2010EvaluationOP}.

\subsection{Uczenie}
%https://medium.com/free-code-camp/building-a-3-layer-neural-network-from-scratch-99239c4af5d3
Proces uczenia sieci neuronowej dzieli się na epoki. Liczba epok jest regulowalnym parametrem i od przyjętej wartości zależy jakość działania modelu. Zbyt mała liczba epok skutkuje niedotrenowaniem (model mógłby klasyfikować lepiej), a zbyt duża przetrenowaniem (model zna zbiór na którym trenował bardzo dobrze, ale słabo radzi sobie z nowymi zbiorami). Poniżej przedstawiono kolejne procesy zachodzące podczas jednej epoki.

Początkowo ustalane są wagi sieci $W_k$ i błędów $b_k$ przez inicjalizację małymi liczbami losowymi. W pierwszej części treningu odbywa się propagacja w przód (ang. \textit{forward popagation)}, która polega na przejściu przez sieć w kierunku od wejścia do wyjścia i obliczeniu liniowego kroku:

\begin{equation}
y_1 = X_0 W_1 + b_1
\end{equation}
gdzie,
\begin{eqwhere}[2cm]
	\item[$X_1$] macierz wejściowa,
	\item[$W_1$] macierz wag,
	\item[$b_1$] błąd (ang. \textit{bias}),
	\item[$y_1$] pierwszy liniowy krok.
\end{eqwhere}

Następnie zbiór liniowych kroków przechodzi przez funkcje aktywacyjne, wprowadzając do modelu cechy nieliniowe i pozwalając na reprezentację bardziej skomplikowanych odwzorowań.

Po zakończonej propagacji w przód następuje etap propagacji wstecznej (ang. \textit{backward propagation}), mający na celu poprawę wartości wag. Na podstwie funkcji błędu - różnicy między wyjściem z modelu (predykcją), a oczekiwanym wyjściem - szacuje się jakość rozwiązania. Używając pochodnej funkcji błędu względem wag minimalizuje się błąd metodą najszybszego spadku. Krok spadku jest determinowany przez parametr nazywany tempem uczenia (ang. \textit{learning rate}). 

Najczęsciej nie wszystkie dane przepływają przez sieć jednocześnie. W przypadku dużych zbiorów danych dzieli się je na mniejsze podzbiory (ang. \textit{batches}), które przepływają kolejno przez sieć. Liczebność tego typu podzbioru jest parametrem modelu i wpływa na jakość klasyfikacji. Liczba iteracji definiuje ile podzbiorów ma przejść przez sieć od wejścia do wyjścia układu i spowrotem, aby epoka została uznana za skończoną. 

\subsection{Sieci pretrenowane - opcjonalnie}
%https://www.analyticsvidhya.com/blog/2018/07/top-10-pretrained-models-get-started-deep-learning-part-1-computer-vision/

%oraz: https://becominghuman.ai/how-do-pretrained-models-work-11fe2f64eaa2
Trening sieci neuronowej jest procesem czasochłonnym. Co więcej, wymaga zgromadzenia odpowiedniej ilości opisanych danych, co bywa problematyczne. Z tego powodu zaczęto szukać metod, dzięki którym będzie można ten proces uprościć i stosować te same narzędzia do różnych problemów. Przenoszenie uczenia (ang. \textit{transfer learning}) jest stosowane w sieciach neronowych przez użycie pretrenowanych modeli. Tego typu model jest trenowany na dużym zbiorze danych i zawierającym nawet kilka milionów próbek i kilkadziesiąt tysięcy klas. 

Korzystając z faktu, że coraz głębsze warstwy sieci uczą się i rozpoznają coraz bardziej skomplikowane i szczegółowe wzorce na obrazie można zedytować raz przetrenowany model do przeznaczenia ogólnego. Należy zamrozić początkowe warstwy - rozpoznające generyczne wzorce - aby nie nadpisać ich wag oraz na nich dołożyć kolejne warstwy mające za zadanie nauczenie się szczegółów typowych dla konkretnego zbioru zdjęć. Dzięki temu można użyć pretrenowanego modelu do rozpoznawania kształtów w bazie zdjęć niezwiązanych wcale z oryginalnym zbiorem, na którym został przetrenowany. 

%MobileNet:
%https://kopernio.com/viewer?doi=arXiv:1704.04861&route=6

\section{Algorytmy klasyfikacji elementów morfologicznych}

%Kaggle kernel