 
results_1.png:
validation_set_percentage = 90%
datagen for train, test and validation:
target_size=(320, 240)
batch_size=20
network:
4 conv layers (3,3), activtion='relu' with MaxPooling (2,2)
1 hidden dense layer with 512 hidden units and relu activation
loss and optimizer:
categorical_crossentropy and RMSprop(lr=1e-4), metrics ['acc']
123s/step - loss: 0.3880 - acc: 0.8580 - val_loss: 0.4019 - val_acc: 0.8072

results_2.png:
the same as results_3, less conv layers (3,3) (3 instead of 4)
39s 390ms/step - loss: 0.4159 - acc: 0.8280 - val_loss: 0.4363 - val_acc: 0.7992

results_3.png:
the same as results_1, but decreased size by half - target_size=(160, 120)
28s 285ms/step - loss: 0.4522 - acc: 0.8060 - val_loss: 0.4831 - val_acc: 0.7761

results_4.png:
the same as results_3, but with augmentation to decrease overftting
(we didn't observe overfitting in our case);
rotation_range=40,
width_shift_range=0.2,
height_shift_range=0.2,
shear_range=0.2,
zoom_range=0.2,
horizontal_flip=True,
fill_mode='nearest'
30s 302ms/step - loss: 0.7291 - acc: 0.6990 - val_loss: 0.6582 - val_acc: 0.7299

results_5.png:
the same as results_4, but with additional 
        model.add(layers.MaxPooling2D((2, 2)))
        model.add(layers.Conv2D(64, (3, 3), activation='relu'))
layer
32s 319ms/step - loss: 0.8420 - acc: 0.6225 - val_loss: 0.8320 - val_acc: 0.6235

results_6.png:
the same as results_4, but with hidden units = 1024
32s 318ms/step - loss: 0.7126 - acc: 0.7055 - val_loss: 0.6578 - val_acc: 0.7229

results_7.png:
the same as results_4, but epochs were extended to 40
32s 325ms/step - loss: 0.5668 - acc: 0.7470 - val_loss: 0.6017 - val_acc: 0.7380

results_8.png:
the same as results_1, but with 3 conv layers

results_8.png:
the same as results_2, but with original image size 320x240 (but there stared to appear overfitting)
121s 1s/step - loss: 0.3091 - acc: 0.8685 - val_loss: 0.3569 - val_acc: 0.8504
